{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f51a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from debiasm import DebiasMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c9d9e",
   "metadata": {},
   "source": [
    "## Build synthetic data for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb97a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n_samples = 96*5\n",
    "n_batches = 5\n",
    "n_features = 100\n",
    "\n",
    "## the read count matrix\n",
    "X = ( np.random.rand(n_samples, n_features) * 1000 ).astype(int)\n",
    "\n",
    "## the labels\n",
    "y = np.random.rand(n_samples)>0.5\n",
    "\n",
    "## the batches\n",
    "batches = ( np.random.rand(n_samples) * n_batches ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452f2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4, 696, 286, 226, 551],\n",
       "       [  4, 513, 666, 105, 130],\n",
       "       [  3, 542,  66, 653, 996],\n",
       "       [  3,  16, 721,   7,  84],\n",
       "       [  1, 456, 279, 932, 314]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we assume the batches are numbered ints starting at '0',\n",
    "## and they are in the first column of the input X matrices\n",
    "## for now, you can just set the first column to all zeros if we have only one batch\n",
    "\n",
    "X_with_batch = np.hstack((batches[:, np.newaxis], X))\n",
    "X_with_batch[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20227111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True,  True])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1db1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the valdiation batch to '4'\n",
    "val_inds = batches==4\n",
    "X_train, X_val = X_with_batch[~val_inds], X_with_batch[val_inds]\n",
    "y_train, y_val = y[~val_inds], y[val_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef3bc2",
   "metadata": {},
   "source": [
    "## run DEBIAS-M, using standard sklearn object formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd90370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0cc1090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cc13a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/george/opt/anaconda3/envs/pl2/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "/Users/george/opt/anaconda3/envs/pl2/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:168: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
      "  \"Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/george/opt/anaconda3/envs/pl2/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/george/Desktop/sandbox/sandbox2/debias-dev/DEBIAS-M/debiasm/torch_functions.py:75: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_hat = softmax(x)\n",
      "/Users/george/opt/anaconda3/envs/pl2/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/Users/george/opt/anaconda3/envs/pl2/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n",
      "Trainer was signaled to stop but required minimum epochs (25) or minimum steps (None) has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training!\n"
     ]
    }
   ],
   "source": [
    "dmc = DebiasMClassifier(x_val=X_val) ## give it the held-out inputs to account for\n",
    "                                    ## those domains shifts while training\n",
    "\n",
    "dmc.fit(X_train, y_train)\n",
    "print('finished training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8321961",
   "metadata": {},
   "source": [
    "## Assess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbea7cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46737967914438505"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_val, dmc.predict_proba(X_val)[:, 1]) \n",
    "## should be ~~0.5 in this notebook , since the data is all random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44f897",
   "metadata": {},
   "source": [
    "## Extract the the 'DEBIAS-ed' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdb3d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01457723, 0.00534167, 0.00494237, 0.01204327, 0.01374663],\n",
       "       [0.01068423, 0.01236932, 0.00228337, 0.00282551, 0.00610285],\n",
       "       [0.01086351, 0.00140875, 0.01448351, 0.02095727, 0.01629219],\n",
       "       [0.00031793, 0.01525674, 0.00015392, 0.00175223, 0.00472576],\n",
       "       [0.00966502, 0.00629951, 0.01727931, 0.00599989, 0.01835283]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_debiassed = dmc.transform(X_with_batch)\n",
    "X_debiassed[:5, :5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl2",
   "language": "python",
   "name": "pl2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
